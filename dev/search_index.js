var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"import AcceleratedKernels as AK # hide\nAK.DocHelpers.readme_section(\"## 10. References\") # hide","category":"page"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"references/","page":"References","title":"References","text":"import AcceleratedKernels as AK # hide\nAK.DocHelpers.readme_section(\"## 11. Acknowledgements\") # hide","category":"page"},{"location":"api/sort/#sort-and-friends","page":"Sorting","title":"sort and friends","text":"","category":"section"},{"location":"api/sort/","page":"Sorting","title":"Sorting","text":"Sorting algorithms with similar interface and default settings as the Julia Base ones, on GPUs:","category":"page"},{"location":"api/sort/","page":"Sorting","title":"Sorting","text":"sort! (in-place), sort (out-of-place)\nsortperm!, sortperm\nOther names: sort, sort_team, sort_team_by_key, stable_sort or variations in Kokkos, RAJA, Thrust that I know of.","category":"page"},{"location":"api/sort/","page":"Sorting","title":"Sorting","text":"Function signatures:","category":"page"},{"location":"api/sort/#AcceleratedKernels.sort!","page":"Sorting","title":"AcceleratedKernels.sort!","text":"sort!(\n    v::AbstractArray, backend::Backend=get_backend(v);\n\n    lt=isless,\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size::Int=256,\n\n    # Temporary buffer, same size as `v`\n    temp::Union{Nothing, AbstractArray}=nothing,\n)\n\nSorts the array v in-place using the specified backend. The lt, by, rev, and order arguments are the same as for Base.sort.\n\nCPU\n\nCPU settings: use at most max_tasks threads to sort the array such that at least min_elems elements are sorted by each thread. A parallel sample_sort! is used, processing independent slices of the array and deferring to Base.sort! for the final local sorts.\n\nNote that the Base Julia sort! is mainly memory-bound, so multithreaded sorting only becomes faster if it is a more compute-heavy operation to hide memory latency - that includes:\n\nSorting more complex types, e.g. lexicographic sorting of tuples / structs / strings.\nMore complex comparators, e.g. by=custom_complex_function or lt=custom_lt_function.\nLess cache-predictable data movement, e.g. sortperm.\n\nGPU\n\nGPU settings: use block_size threads per block to sort the array. A parallel merge_sort! is used.\n\nFor both CPU and GPU backends, the temp argument can be used to reuse a temporary buffer of the same size as v to store the sorted output.\n\nExamples\n\nSimple parallel CPU sort using all available threads (as given by julia --threads N):\n\nimport AcceleratedKernels as AK\nv = rand(1000)\nAK.sort!(v)\n\nParallel GPU sorting, passing a temporary buffer to avoid allocating a new one:\n\nusing oneAPI\nimport AcceleratedKernels as AK\nv = oneArray(rand(1000))\ntemp = similar(v)\nAK.sort!(v, temp=temp)\n\n\n\n\n\n","category":"function"},{"location":"api/sort/#AcceleratedKernels.sort","page":"Sorting","title":"AcceleratedKernels.sort","text":"sort(\n    v::AbstractArray, backend::Backend=get_backend(v);\n\n    lt=isless,\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size::Int=256,\n\n    # Temporary buffer, same size as `v`\n    temp::Union{Nothing, AbstractArray}=nothing,\n)\n\nOut-of-place sort, same settings as sort!.\n\n\n\n\n\n","category":"function"},{"location":"api/sort/#AcceleratedKernels.sortperm!","page":"Sorting","title":"AcceleratedKernels.sortperm!","text":"sortperm!(\n    ix::AbstractArray,\n    v::AbstractArray,\n    backend::Backend=get_backend(v);\n\n    lt=isless,\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size::Int=256,\n\n    # Temporary buffer, same size as `v`\n    temp::Union{Nothing, AbstractArray}=nothing,\n)\n\nSave into ix the index permutation of v such that v[ix] is sorted. The lt, by, rev, and order arguments are the same as for Base.sortperm. The same algorithms are used as for sort! with custom by-index comparators.\n\n\n\n\n\n","category":"function"},{"location":"api/sort/#AcceleratedKernels.sortperm","page":"Sorting","title":"AcceleratedKernels.sortperm","text":"sortperm(\n    v::AbstractArray,\n    backend::Backend=get_backend(v);\n\n    lt=isless,\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size::Int=256,\n\n    # Temporary buffer, same size as `v`\n    temp::Union{Nothing, AbstractArray}=nothing,\n)\n\nOut-of-place sortperm, same settings as sortperm!.\n\n\n\n\n\n","category":"function"},{"location":"api/sort/","page":"Sorting","title":"Sorting","text":"Specific implementations that the interfaces above forward to:","category":"page"},{"location":"api/sort/","page":"Sorting","title":"Sorting","text":"sample_sort! - multithreaded CPU sample sort, deferring to Base.sort! on independent slices.\nmerge_sort! (in-place), merge_sort (out-of-place) - sort arbitrary objects with custom comparisons.\nmerge_sort_by_key!, merge_sort_by_key - sort a vector of keys along with a \"payload\", a vector of corresponding values.\nmerge_sortperm!, merge_sortperm, merge_sortperm_lowmem!, merge_sortperm_lowmem - compute a sorting index permutation. ","category":"page"},{"location":"api/sort/","page":"Sorting","title":"Sorting","text":"Function signatures:","category":"page"},{"location":"api/sort/#AcceleratedKernels.sample_sort!","page":"Sorting","title":"AcceleratedKernels.sample_sort!","text":"sample_sort!(\n    v::AbstractArray;\n\n    lt=isless,\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n    temp::Union{Nothing, AbstractArray}=nothing,\n)\n\n\n\n\n\n","category":"function"},{"location":"api/sort/#AcceleratedKernels.sample_sortperm!","page":"Sorting","title":"AcceleratedKernels.sample_sortperm!","text":"sample_sortperm!(\n    ix::AbstractArray, v::AbstractArray;\n\n    lt=isless,\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n    temp::Union{Nothing, AbstractArray}=nothing,\n)\n\n\n\n\n\n","category":"function"},{"location":"api/sort/#AcceleratedKernels.merge_sort!","page":"Sorting","title":"AcceleratedKernels.merge_sort!","text":"merge_sort!(\n    v::AbstractArray, backend::Backend=get_backend(v);\n\n    lt=isless,\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n)\n\n\n\n\n\n","category":"function"},{"location":"api/sort/#AcceleratedKernels.merge_sort","page":"Sorting","title":"AcceleratedKernels.merge_sort","text":"merge_sort(\n    v::AbstractArray, backend::Backend=get_backend(v);\n\n    lt=isless,\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n)\n\n\n\n\n\n","category":"function"},{"location":"api/sort/#AcceleratedKernels.merge_sort_by_key!","page":"Sorting","title":"AcceleratedKernels.merge_sort_by_key!","text":"merge_sort_by_key!(\n    keys::AbstractArray,\n    values::AbstractArray,\n    backend::Backend=get_backend(keys);\n\n    lt=isless,\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    block_size::Int=256,\n    temp_keys::Union{Nothing, AbstractArray}=nothing,\n    temp_values::Union{Nothing, AbstractArray}=nothing,\n)\n\n\n\n\n\n","category":"function"},{"location":"api/sort/#AcceleratedKernels.merge_sort_by_key","page":"Sorting","title":"AcceleratedKernels.merge_sort_by_key","text":"merge_sort_by_key(\n    keys::AbstractArray,\n    values::AbstractArray,\n    backend::Backend=get_backend(keys);\n\n    lt=isless,\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    block_size::Int=256,\n    temp_keys::Union{Nothing, AbstractArray}=nothing,\n    temp_values::Union{Nothing, AbstractArray}=nothing,\n)\n\n\n\n\n\n","category":"function"},{"location":"api/sort/#AcceleratedKernels.merge_sortperm!","page":"Sorting","title":"AcceleratedKernels.merge_sortperm!","text":"merge_sortperm!(\n    ix::AbstractArray,\n    v::AbstractArray,\n    backend::Backend=get_backend(v);\n\n    lt=(<),\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    inplace::Bool=false,\n    block_size::Int=256,\n    temp_ix::Union{Nothing, AbstractArray}=nothing,\n    temp_v::Union{Nothing, AbstractArray}=nothing,\n)\n\n\n\n\n\n","category":"function"},{"location":"api/sort/#AcceleratedKernels.merge_sortperm","page":"Sorting","title":"AcceleratedKernels.merge_sortperm","text":"merge_sortperm(\n    v::AbstractArray, backend::Backend=get_backend(v);\n\n    lt=(<),\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    inplace::Bool=false,\n    block_size::Int=256,\n    temp_ix::Union{Nothing, AbstractArray}=nothing,\n    temp_v::Union{Nothing, AbstractArray}=nothing,\n)\n\n\n\n\n\n","category":"function"},{"location":"api/sort/#AcceleratedKernels.merge_sortperm_lowmem!","page":"Sorting","title":"AcceleratedKernels.merge_sortperm_lowmem!","text":"merge_sortperm_lowmem!(\n    ix::AbstractArray,\n    v::AbstractArray,\n    backend::Backend=get_backend(v);\n\n    lt=(<),\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n)\n\n\n\n\n\n","category":"function"},{"location":"api/sort/#AcceleratedKernels.merge_sortperm_lowmem","page":"Sorting","title":"AcceleratedKernels.merge_sortperm_lowmem","text":"merge_sortperm_lowmem(\n    v::AbstractArray, backend::Backend=get_backend(v);\n\n    lt=(<),\n    by=identity,\n    rev::Union{Nothing, Bool}=nothing,\n    order::Base.Order.Ordering=Base.Order.Forward,\n\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n)\n\n\n\n\n\n","category":"function"},{"location":"api/sort/","page":"Sorting","title":"Sorting","text":"Example:","category":"page"},{"location":"api/sort/","page":"Sorting","title":"Sorting","text":"import AcceleratedKernels as AK\nusing AMDGPU\n\nv = ROCArray(rand(Int32, 100_000))\nAK.sort!(v)","category":"page"},{"location":"api/sort/","page":"Sorting","title":"Sorting","text":"As GPU memory is more expensive, all functions in AcceleratedKernels.jl expose any temporary arrays they will use (the temp argument); you can supply your own buffers to make the algorithms not allocate additional GPU storage, e.g.:","category":"page"},{"location":"api/sort/","page":"Sorting","title":"Sorting","text":"v = ROCArray(rand(Float32, 100_000))\ntemp = similar(v)\nAK.sort!(v, temp=temp)","category":"page"},{"location":"api/accumulate/#Accumulate-/-Prefix-Sum-/-Scan","page":"Accumulate","title":"Accumulate / Prefix Sum / Scan","text":"","category":"section"},{"location":"api/accumulate/#AcceleratedKernels.accumulate!","page":"Accumulate","title":"AcceleratedKernels.accumulate!","text":"accumulate!(\n    op, v::AbstractArray, backend::Backend=get_backend(v);\n    init,\n    neutral=neutral_element(op, eltype(v)),\n    dims::Union{Nothing, Int}=nothing,\n    inclusive::Bool=true,\n\n    # CPU settings\n    max_tasks::Int=Threads.nthreads(),\n    min_elems::Int=2,\n\n    # Algorithm choice\n    alg::AccumulateAlgorithm=DecoupledLookback(),\n\n    # GPU settings\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n    temp_flags::Union{Nothing, AbstractArray}=nothing,\n)\n\naccumulate!(\n    op, dst::AbstractArray, src::AbstractArray, backend::Backend=get_backend(v);\n    init,\n    neutral=neutral_element(op, eltype(dst)),\n    dims::Union{Nothing, Int}=nothing,\n    inclusive::Bool=true,\n\n    # CPU settings\n    max_tasks::Int=Threads.nthreads(),\n    min_elems::Int=2,\n\n    # Algorithm choice\n    alg::AccumulateAlgorithm=DecoupledLookback(),\n\n    # GPU settings\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n    temp_flags::Union{Nothing, AbstractArray}=nothing,\n)\n\nCompute accumulated running totals along a sequence by applying a binary operator to all elements up to the current one; often used in GPU programming as a first step in finding / extracting subsets of data.\n\nOther names: prefix sum, thrust::scan, cumulative sum; inclusive (or exclusive) if the first element is included in the accumulation (or not).\n\nFor compatibility with the Base.accumulate! function, we provide the two-array interface too, but we do not need the constraint of dst and src being different; to minimise memory use, we recommend using the single-array interface (the first one above).\n\nCPU\n\nUse at most max_tasks threads with at least min_elems elements per task.\n\nNote that accumulation is typically a memory-bound operation, so multithreaded accumulation only becomes faster if it is a more compute-heavy operation to hide memory latency - that includes:\n\nAccumulating more complex types, e.g. accumulation of tuples / structs / strings.\nMore complex operators, e.g. op=custom_complex_function.\n\nGPU\n\nFor the 1D case (dims=nothing), the alg can be one of the following:\n\nDecoupledLookback(): the default algorithm, using opportunistic lookback to reuse earlier blocks' results; requires device-level memory consistency guarantees, which Apple Metal does not provide.\nScanPrefixes(): a simpler algorithm that scans the prefixes of each block, with no lookback; it has similar performance as DecoupledLookback() for large block sizes, and small to medium arrays, but poorer scaling for many blocks; there is no performance degradation below block_size^2 elements.\n\nA different, unique algorithm is used for the multi-dimensional case (dims is an integer).\n\nThe block_size should be a power of 2 and greater than 0.\n\nThe temporaries are only used for the 1D case (dims=nothing): temp stores per-block aggregates; temp_flags is only used for the DecoupledLookback() algorithm for flagging if blocks are ready; they should both have at least (length(v) + 2 * block_size - 1) ÷ (2 * block_size) elements; also, eltype(v) === eltype(temp) is required; the elements in temp_flags can be any integers, but Int8 is used by default to reduce memory usage.\n\nPlatform-Specific Notes\n\nOn Metal, the alg=ScanPrefixes() algorithm is used by default, as Apple Metal GPUs do not have strong enough memory consistency guarantees for the DecoupledLookback() algorithm - which produces incorrect results about 0.38% of the time (the beauty of parallel algorithms, ey). Also, block_size=1024 is used here by default to reduce the number of coupled lookbacks.\n\nExamples\n\nExample computing an inclusive prefix sum (the typical GPU \"scan\"):\n\nimport AcceleratedKernels as AK\nusing oneAPI\n\nv = oneAPI.ones(Int32, 100_000)\nAK.accumulate!(+, v, init=0)\n\n# Use a different algorithm\nAK.accumulate!(+, v, alg=AK.ScanPrefixes())\n\n\n\n\n\n","category":"function"},{"location":"api/accumulate/#AcceleratedKernels.accumulate","page":"Accumulate","title":"AcceleratedKernels.accumulate","text":"accumulate(\n    op, v::AbstractArray, backend::Backend=get_backend(v);\n    init,\n    neutral=neutral_element(op, eltype(v)),\n    dims::Union{Nothing, Int}=nothing,\n    inclusive::Bool=true,\n\n    # CPU settings\n    max_tasks::Int=Threads.nthreads(),\n    min_elems::Int=2,\n\n    # Algorithm choice\n    alg::AccumulateAlgorithm=DecoupledLookback(),\n\n    # GPU settings\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n    temp_flags::Union{Nothing, AbstractArray}=nothing,\n)\n\nOut-of-place version of accumulate!.\n\n\n\n\n\n","category":"function"},{"location":"api/task_partition/#Multithreaded-Task-Partitioning","page":"Task Partitioning","title":"Multithreaded Task Partitioning","text":"","category":"section"},{"location":"api/task_partition/#AcceleratedKernels.TaskPartitioner","page":"Task Partitioning","title":"AcceleratedKernels.TaskPartitioner","text":"Partitioning num_elems elements / jobs over maximum max_tasks tasks with minimum min_elems elements per task.\n\nMethods\n\nTaskPartitioner(num_elems, max_tasks=Threads.nthreads(), min_elems=1)\nBase.getindex(tp::TaskPartitioner, itask::Integer)\nBase.firstindex(tp::TaskPartitioner)\nBase.lastindex(tp::TaskPartitioner)\nBase.length(tp::TaskPartitioner)\n\nFields\n\nnum_elems::Int : (user-defined) number of elements / jobs to partition.\nmax_tasks::Int : (user-defined) maximum number of tasks to use.\nmin_elems::Int : (user-defined) minimum number of elements per task.\nnum_tasks::Int : (computed) number of tasks actually needed.\ntask_istarts::Vector{Int} : (computed) element starting index for each task.\ntasks::Vector{Task} : (computed) array of tasks; can be reused.\n\nExamples\n\nusing AcceleratedKernels: TaskPartitioner\n\n# Divide 10 elements between 4 tasks\ntp = TaskPartitioner(10, 4)\nfor i in 1:tp.num_tasks\n    @show tp[i]\nend\n\n# output\ntp[i] = 1:3\ntp[i] = 4:6\ntp[i] = 7:8\ntp[i] = 9:10\n\nusing AcceleratedKernels: TaskPartitioner\n\n# Divide 20 elements between 6 tasks with minimum 5 elements per task.\n# Not all tasks will be required\ntp = TaskPartitioner(20, 6, 5)\nfor i in 1:tp.num_tasks\n    @show tp[i]\nend\n\n# output\ntp[i] = 1:5\ntp[i] = 6:10\ntp[i] = 11:15\ntp[i] = 16:20\n\nThe TaskPartitioner is used internally by task_partition and itask_partition; you can construct one manually if you want to reuse the same partitioning for multiple tasks - this also reuses the tasks array and minimises allocations.\n\n\n\n\n\n","category":"type"},{"location":"api/task_partition/#AcceleratedKernels.task_partition","page":"Task Partitioning","title":"AcceleratedKernels.task_partition","text":"task_partition(f, num_elems, max_tasks=Threads.nthreads(), min_elems=1)\ntask_partition(f, tp::TaskPartitioner)\n\nPartition num_elems jobs across at most num_tasks parallel tasks with at least min_elems per task, calling f(start_index:end_index), where the indices are between 1 and num_elems.\n\nExamples\n\nA toy example showing outputs:\n\nnum_elems = 4\ntask_partition(println, num_elems)\n\n# Output, possibly in a different order due to threading order\n1:1\n4:4\n2:2\n3:3\n\nThis function is probably most useful with a do-block, e.g.:\n\ntask_partition(4) do irange\n    some_long_computation(param1, param2, irange)\nend\n\nThe TaskPartitioner form allows you to reuse the same partitioning for multiple tasks - this also reuses the tasks array and minimises allocations:\n\ntp = TaskPartitioner(4)\ntask_partition(tp) do irange\n    some_long_computation(param1, param2, irange)\nend\n# Reuse same partitioning and tasks array\ntask_partition(tp) do irange\n    some_other_long_computation(param1, param2, irange)\nend\n\n\n\n\n\n","category":"function"},{"location":"api/task_partition/#AcceleratedKernels.itask_partition","page":"Task Partitioning","title":"AcceleratedKernels.itask_partition","text":"itask_partition(f, num_elems, max_tasks=Threads.nthreads(), min_elems=1)\nitask_partition(f, tp::TaskPartitioner)\n\nPartition num_elems jobs across at most num_tasks parallel tasks with at least min_elems per task, calling f(itask, start_index:end_index), where the indices are between 1 and num_elems.\n\nExamples\n\nA toy example showing outputs:\n\nnum_elems = 4\nitask_partition(num_elems) do itask, irange\n    @show (itask, irange)\nend\n\n# Output, possibly in a different order due to threading order\n(itask, irange) = (3, 3:3)\n(itask, irange) = (1, 1:1)\n(itask, irange) = (2, 2:2)\n(itask, irange) = (4, 4:4)\n\nThis function is probably most useful with a do-block, e.g.:\n\ntask_partition(4) do itask, irange\n    some_long_computation_needing_itask(param1, param2, irange)\nend\n\nThe TaskPartitioner form allows you to reuse the same partitioning for multiple tasks - this also reuses the tasks array and minimises allocations:\n\ntp = TaskPartitioner(4)\nitask_partition(tp) do itask, irange\n    some_long_computation_needing_itask(param1, param2, irange)\nend\n# Reuse same partitioning and tasks array\nitask_partition(tp) do itask, irange\n    some_other_long_computation_needing_itask(param1, param2, irange)\nend\n\n\n\n\n\n","category":"function"},{"location":"api/foreachindex/#General-Looping:-foreachindex-/-foraxes","page":"General Loops","title":"General Looping: foreachindex / foraxes","text":"","category":"section"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"General workhorses for converting normal Julia for loops into GPU code, for example:","category":"page"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"# Copy kernel testing throughput\nfunction cpu_copy!(dst, src)\n    for i in eachindex(src)\n        dst[i] = src[i]\n    end\nend","category":"page"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"Would be written for GPU as:","category":"page"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"import AcceleratedKernels as AK\n\nfunction gpu_copy!(dst, src)\n    AK.foreachindex(src) do i\n        dst[i] = src[i]\n    end\nend","category":"page"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"Yes, simply change for i in eachindex(itr) into AK.foreachindex(itr) do i to run it on GPUs / multithreaded - magic! (or just amazing language design)","category":"page"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"This is a parallelised for-loop over the indices of an iterable; converts normal Julia code to GPU kernels running one thread per index. On CPUs it executes static index ranges on max_tasks threads, with user-defined min_elems to be processed by each thread; if only a single thread ends up being needed, the loop is inlined and executed without spawning threads.","category":"page"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"Other names: Kokkos::parallel_for, RAJA::forall, thrust::transform.","category":"page"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"Example:","category":"page"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"import AcceleratedKernels as AK\n\nfunction f(a, b)\n    # Don't use global arrays inside a `foreachindex`; types must be known\n    @assert length(a) == length(b)\n    AK.foreachindex(a) do i\n        # Note that we don't have to explicitly pass b into the lambda\n        if b[i] > 0.5\n            a[i] = 1\n        else\n            a[i] = 0\n        end\n\n        # Showing arbitrary if conditions; can also be written as:\n        # @inbounds a[i] = b[i] > 0.5 ? 1 : 0\n    end\nend\n\n# Use any backend, e.g. CUDA, ROCm, oneAPI, Metal, or CPU\nusing oneAPI\nv1 = oneArray{Float32}(undef, 100_000)\nv2 = oneArray(rand(Float32, 100_000))\nf(v1, v2)","category":"page"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"All GPU functions allow you to specify a block size - this is often a power of two (mostly 64, 128, 256, 512); the optimum depends on the algorithm, input data and hardware - you can try the different values and @time or @benchmark them:","category":"page"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"@time AK.foreachindex(f, itr_gpu, block_size=512)","category":"page"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"Similarly, for performance on the CPU the overhead of spawning threads should be masked by processing more elements per thread (but there is no reason here to launch more threads than Threads.nthreads(), the number of threads Julia was started with); the optimum depends on how expensive f is - again, benchmarking is your friend:","category":"page"},{"location":"api/foreachindex/","page":"General Loops","title":"General Loops","text":"@time AK.foreachindex(f, itr_cpu, max_tasks=16, min_elems=1000)","category":"page"},{"location":"api/foreachindex/#AcceleratedKernels.foreachindex","page":"General Loops","title":"AcceleratedKernels.foreachindex","text":"foreachindex(\n    f, itr, backend::Backend=get_backend(itr);\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size=256,\n)\n\nParallelised for loop over the indices of an iterable.\n\nIt allows you to run normal Julia code on a GPU over multiple arrays - e.g. CuArray, ROCArray, MtlArray, oneArray - with one GPU thread per index.\n\nOn CPUs at most max_tasks threads are launched, or fewer such that each thread processes at least min_elems indices; if a single task ends up being needed, f is inlined and no thread is launched. Tune it to your function - the more expensive it is, the fewer elements are needed to amortise the cost of launching a thread (which is a few μs).\n\nExamples\n\nNormally you would write a for loop like this:\n\nfunction f()\n    x = Array(1:100)\n    y = similar(x)\n    for i in eachindex(x)\n        @inbounds y[i] = 2 * x[i] + 1\n    end\nend\n\nUsing this function you can have the same for loop body over a GPU array:\n\nusing CUDA\nimport AcceleratedKernels as AK\n\nfunction f()\n    x = CuArray(1:100)\n    y = similar(x)\n    AK.foreachindex(x) do i\n        @inbounds y[i] = 2 * x[i] + 1\n    end\nend\n\nNote that the above code is pure arithmetic, which you can write directly (and on some platforms it may be faster) as:\n\nusing CUDA\nx = CuArray(1:100)\ny = 2 .* x .+ 1\n\nImportant note: to use this function on a GPU, the objects referenced inside the loop body must have known types - i.e. be inside a function. For example:\n\nusing oneAPI\nimport AcceleratedKernels as AK\n\nx = oneArray(1:100)\n\n# CRASHES - typical error message: \"Reason: unsupported dynamic function invocation\"\n# AK.foreachindex(x) do i\n#     x[i] = i\n# end\n\nfunction somecopy!(v)\n    # Because it is inside a function, the type of `v` will be known\n    AK.foreachindex(v) do i\n        v[i] = i\n    end\nend\n\nsomecopy!(x)    # This works\n\n\n\n\n\n","category":"function"},{"location":"api/foreachindex/#AcceleratedKernels.foraxes","page":"General Loops","title":"AcceleratedKernels.foraxes","text":"foraxes(\n    f, itr, dims::Union{Nothing, <:Integer}=nothing, backend::Backend=get_backend(itr);\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size=256,\n)\n\nParallelised for loop over the indices along axis dims of an iterable.\n\nIt allows you to run normal Julia code on a GPU over multiple arrays - e.g. CuArray, ROCArray, MtlArray, oneArray - with one GPU thread per index.\n\nOn CPUs at most max_tasks threads are launched, or fewer such that each thread processes at least min_elems indices; if a single task ends up being needed, f is inlined and no thread is launched. Tune it to your function - the more expensive it is, the fewer elements are needed to amortise the cost of launching a thread (which is a few μs).\n\nExamples\n\nNormally you would write a for loop like this:\n\nfunction f()\n    x = Array(reshape(1:30, 3, 10))\n    y = similar(x)\n    for i in axes(x, 2)\n        for j in axes(x, 1)\n            @inbounds y[j, i] = 2 * x[j, i] + 1\n        end\n    end\nend\n\nUsing this function you can have the same for loop body over a GPU array:\n\nusing CUDA\nimport AcceleratedKernels as AK\n\nfunction f()\n    x = CuArray(reshape(1:3000, 3, 1000))\n    y = similar(x)\n    AK.foraxes(x, 2) do i\n        for j in axes(x, 1)\n            @inbounds y[j, i] = 2 * x[j, i] + 1\n        end\n    end\nend\n\nImportant note: to use this function on a GPU, the objects referenced inside the loop body must have known types - i.e. be inside a function. For example:\n\nusing oneAPI\nimport AcceleratedKernels as AK\n\nx = oneArray(reshape(1:3000, 3, 1000))\n\n# CRASHES - typical error message: \"Reason: unsupported dynamic function invocation\"\n# AK.foraxes(x) do i\n#     x[i] = i\n# end\n\nfunction somecopy!(v)\n    # Because it is inside a function, the type of `v` will be known\n    AK.foraxes(v) do i\n        v[i] = i\n    end\nend\n\nsomecopy!(x)    # This works\n\n\n\n\n\n","category":"function"},{"location":"api/map/#Map","page":"Map","title":"Map","text":"","category":"section"},{"location":"api/map/#AcceleratedKernels.map!","page":"Map","title":"AcceleratedKernels.map!","text":"map!(\n    f, dst::AbstractArray, src::AbstractArray, backend::Backend=get_backend(src);\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size=256,\n)\n\nApply the function f to each element of src in parallel and store the result in dst. The CPU and GPU settings are the same as for foreachindex.\n\nOn CPUs, multithreading only improves performance when complex computation hides the memory latency and the overhead of spawning tasks - that includes more complex functions and less cache-local array access patterns. For compute-bound tasks, it scales linearly with the number of threads.\n\nExamples\n\nimport Metal\nimport AcceleratedKernels as AK\n\nx = MtlArray(rand(Float32, 100_000))\ny = similar(x)\nAK.map!(y, x) do x_elem\n    T = typeof(x_elem)\n    T(2) * x_elem + T(1)\nend\n\n\n\n\n\n","category":"function"},{"location":"api/map/#AcceleratedKernels.map","page":"Map","title":"AcceleratedKernels.map","text":"map(\n    f, src::AbstractArray, backend::Backend=get_backend(src);\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size=256,\n)\n\nApply the function f to each element of src and store the results in a copy of src (if f changes the eltype, allocate dst separately and call map!). The CPU and GPU settings are the same as for foreachindex.\n\n\n\n\n\n","category":"function"},{"location":"api/binarysearch/#Binary-Search","page":"Binary Search","title":"Binary Search","text":"","category":"section"},{"location":"api/binarysearch/","page":"Binary Search","title":"Binary Search","text":"Find the indices where some elements x should be inserted into a sorted sequence v to maintain the sorted order. Effectively applying the Julia.Base functions in parallel on a GPU using foreachindex.","category":"page"},{"location":"api/binarysearch/","page":"Binary Search","title":"Binary Search","text":"searchsortedfirst! (in-place), searchsortedfirst (allocating): index of first element in v >= x[j].\nsearchsortedlast!, searchsortedlast: index of last element in v <= x[j].\nOther names: thrust::upper_bound, std::lower_bound.","category":"page"},{"location":"api/binarysearch/","page":"Binary Search","title":"Binary Search","text":"Example:","category":"page"},{"location":"api/binarysearch/","page":"Binary Search","title":"Binary Search","text":"import AcceleratedKernels as AK\nusing Metal\n\n# Sorted array\nv = MtlArray(rand(Float32, 100_000))\nAK.merge_sort!(v)\n\n# Elements `x` to place within `v` at indices `ix`\nx = MtlArray(rand(Float32, 10_000))\nix = MtlArray{Int}(undef, 10_000)\n\nAK.searchsortedfirst!(ix, v, x)","category":"page"},{"location":"api/binarysearch/#AcceleratedKernels.searchsortedfirst!","page":"Binary Search","title":"AcceleratedKernels.searchsortedfirst!","text":"searchsortedfirst!(\n    ix::AbstractVector,\n    v::AbstractVector,\n    x::AbstractVector,\n    backend::Backend=get_backend(x);\n\n    by=identity, lt=isless, rev::Bool=false,\n\n    # CPU settings\n    max_tasks::Int=Threads.nthreads(),\n    min_elems::Int=1000,\n\n    # GPU settings\n    block_size::Int=256,\n)\n\nEquivalent to applying searchsortedfirst! element-wise to each element of x. The CPU and GPU settings are the same as for foreachindex.\n\n\n\n\n\n","category":"function"},{"location":"api/binarysearch/#AcceleratedKernels.searchsortedfirst","page":"Binary Search","title":"AcceleratedKernels.searchsortedfirst","text":"searchsortedfirst(\n    v::AbstractVector,\n    x::AbstractVector,\n    backend::Backend=get_backend(x);\n\n    by=identity, lt=isless, rev::Bool=false,\n\n    # CPU settings\n    max_tasks::Int=Threads.nthreads(),\n    min_elems::Int=1000,\n\n    # GPU settings\n    block_size::Int=256,\n)\n\nEquivalent to applying searchsortedfirst element-wise to each element of x. The CPU and GPU settings are the same as for foreachindex.\n\n\n\n\n\n","category":"function"},{"location":"api/binarysearch/#AcceleratedKernels.searchsortedlast!","page":"Binary Search","title":"AcceleratedKernels.searchsortedlast!","text":"searchsortedlast!(\n    ix::AbstractVector,\n    v::AbstractVector,\n    x::AbstractVector,\n    backend::Backend=get_backend(x);\n\n    by=identity, lt=isless, rev::Bool=false,\n\n    # CPU settings\n    max_tasks::Int=Threads.nthreads(),\n    min_elems::Int=1000,\n\n    # GPU settings\n    block_size::Int=256,\n)\n\nEquivalent to applying searchsortedlast! element-wise to each element of x. The CPU and GPU settings are the same as for foreachindex.\n\n\n\n\n\n","category":"function"},{"location":"api/binarysearch/#AcceleratedKernels.searchsortedlast","page":"Binary Search","title":"AcceleratedKernels.searchsortedlast","text":"searchsortedlast(\n    v::AbstractVector,\n    x::AbstractVector,\n    backend::Backend=get_backend(x);\n\n    by=identity, lt=isless, rev::Bool=false,\n\n    # CPU settings\n    max_tasks::Int=Threads.nthreads(),\n    min_elems::Int=1000,\n\n    # GPU settings\n    block_size::Int=256,\n)\n\nEquivalent to applying searchsortedlast element-wise to each element of x. The CPU and GPU settings are the same as for foreachindex.\n\n\n\n\n\n","category":"function"},{"location":"benchmarks/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"import AcceleratedKernels as AK # hide\nAK.DocHelpers.readme_section(\"## 3. Benchmarks\") # hide","category":"page"},{"location":"performance/#Performance-Tips","page":"Performance Tips","title":"Performance Tips","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"If you just started using AcceleratedKernels.jl, see the Manual first for some examples.","category":"page"},{"location":"performance/#GPU-Block-Size-and-CPU-Threads","page":"Performance Tips","title":"GPU Block Size and CPU Threads","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"All GPU functions allow you to specify a block size - this is often a power of two (mostly 64, 128, 256, 512); the optimum depends on the algorithm, input data and hardware - you can try the different values and @time or @benchmark them:","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"@time AK.foreachindex(f, itr_gpu, block_size=512)","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"Similarly, for performance on the CPU the overhead of spawning threads should be masked by processing more elements per thread (but there is no reason here to launch more threads than Threads.nthreads(), the number of threads Julia was started with); the optimum depends on how expensive f is - again, benchmarking is your friend:","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"@time AK.foreachindex(f, itr_cpu, max_tasks=16, min_elems=1000)","category":"page"},{"location":"performance/#Temporary-Arrays","page":"Performance Tips","title":"Temporary Arrays","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"As GPU memory is more expensive, all functions in AcceleratedKernels.jl expose any temporary arrays they will use (the temp argument); you can supply your own buffers to make the algorithms not allocate additional GPU storage, e.g.:","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"v = ROCArray(rand(Float32, 100_000))\ntemp = similar(v)\nAK.sort!(v, temp=temp)","category":"page"},{"location":"api/utilities/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"api/utilities/#AcceleratedKernels.TypeWrap","page":"Utilities","title":"AcceleratedKernels.TypeWrap","text":"struct TypeWrap{T} end\nTypeWrap(T) = TypeWrap{T}()\nBase.:*(x::Number, ::TypeWrap{T}) where T = T(x)\n\nAllow type conversion via multiplication, like 5i32 for 5 * i32 where i32 is a TypeWrap.\n\nExamples\n\nimport AcceleratedKernels as AK\nu32 = AK.TypeWrap{UInt32}\nprintln(typeof(5u32))\n\n# output\nUInt32\n\nThis is used e.g. to set integer literals inside kernels as u16 to ensure no indices are promoted beyond the index base type.\n\nFor example, Metal uses UInt32 indices, but if it is mixed with a Julia integer literal (Int64 by default) like in src[ithread + 1], we incur a type cast to Int64. Instead, we can use src[ithread + 1u16] or src[ithread + 0x1] to ensure the index is UInt32 and avoid the cast; as the integer literal 1u16 has a shorter type than ithread, it is automatically promoted (at compile time) to the ithread type, whether ithread is signed or unsigned as per the backend.\n\n# Defaults defined\n1u8, 2u16, 3u32, 4u64\n5i8, 6i16, 7i32, 8i64\n\n\n\n\n\n","category":"type"},{"location":"api/custom_structs/#Custom-Structs","page":"Custom Structs","title":"Custom Structs","text":"","category":"section"},{"location":"api/custom_structs/","page":"Custom Structs","title":"Custom Structs","text":"import AcceleratedKernels as AK # hide\nAK.DocHelpers.readme_section(\"## 6. Custom Structs\") # hide","category":"page"},{"location":"api/custom_structs/","page":"Custom Structs","title":"Custom Structs","text":"You can also use unmaterialised index ranges in GPU kernels - unmaterialised meaning you do not need to waste memory creating a vector of indices, e.g.:","category":"page"},{"location":"api/custom_structs/","page":"Custom Structs","title":"Custom Structs","text":"import AcceleratedKernels as AK\nusing CUDA\n\nfunction complex_any(x, y)\n    # Calling `any` on a normal Julia range, but running on x's backend\n    AK.any(1:length(x), AK.get_backend(x)) do i\n        x[i] < 0 && y[i] > 0\n    end\nend\n\ncomplex_any(CuArray(rand(Float32, 100)), CuArray(rand(Float32, 100)))","category":"page"},{"location":"api/custom_structs/","page":"Custom Structs","title":"Custom Structs","text":"Note that you have to specify the backend explicitly in this case, as a range does not have a backend per se - for example, when used in a GPU kernel, it only passes two numbers, the Base.UnitRange start and stop, as saved in a basic struct, rather than a whole vector.","category":"page"},{"location":"roadmap/#Roadmap-/-Future-Plans","page":"Roadmap","title":"Roadmap / Future Plans","text":"","category":"section"},{"location":"roadmap/","page":"Roadmap","title":"Roadmap","text":"import AcceleratedKernels as AK # hide\nAK.DocHelpers.readme_section(\"## 9. Roadmap / Future Plans\") # hide","category":"page"},{"location":"api/arithmetics/#Arithmetics","page":"Arithmetics","title":"Arithmetics","text":"","category":"section"},{"location":"api/arithmetics/#AcceleratedKernels.sum","page":"Arithmetics","title":"AcceleratedKernels.sum","text":"sum(\n    src::AbstractArray, backend::Backend=get_backend(src);\n    init=zero(eltype(src)),\n    dims::Union{Nothing, Int}=nothing,\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n    switch_below::Int=0,\n)\n\nSum of elements of an array, with optional init and dims. Arguments are the same as for reduce.\n\nExamples\n\nSimple sum of elements in a vector:\n\nimport AcceleratedKernels as AK\nusing Metal\n\nv = MtlArray(rand(Int32(1):Int32(100), 100_000))\ns = AK.sum(v)\n\nRow-wise sum of a matrix:\n\nm = MtlArray(rand(Int32(1):Int32(100), 10, 100_000))\ns = AK.sum(m, dims=1)\n\nIf you know the shape of the resulting array (in case of a axis-wise sum, i.e. dims is not nothing), you can provide the temp argument to save results into and avoid allocations:\n\nm = MtlArray(rand(Int32(1):Int32(100), 10, 100_000))\ntemp = MtlArray(zeros(Int32, 10))\ns = AK.sum(m, dims=2, temp=temp)\n\n\n\n\n\n","category":"function"},{"location":"api/arithmetics/#AcceleratedKernels.prod","page":"Arithmetics","title":"AcceleratedKernels.prod","text":"prod(\n    src::AbstractArray, backend::Backend=get_backend(src);\n    init=one(eltype(src)),\n    dims::Union{Nothing, Int}=nothing,\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n    switch_below::Int=0,\n)\n\nProduct of elements of an array, with optional init and dims. Arguments are the same as for reduce.\n\nExamples\n\nSimple product of elements in a vector:\n\nimport AcceleratedKernels as AK\nusing AMDGPU\n\nv = ROCArray(rand(Int32(1):Int32(100), 100_000))\np = AK.prod(v)\n\nRow-wise product of a matrix:\n\nm = ROCArray(rand(Int32(1):Int32(100), 10, 100_000))\np = AK.prod(m, dims=1)\n\nIf you know the shape of the resulting array (in case of a axis-wise product, i.e. dims is not nothing), you can provide the temp argument to save results into and avoid allocations:\n\nm = ROCArray(rand(Int32(1):Int32(100), 10, 100_000))\ntemp = ROCArray(ones(Int32, 10))\np = AK.prod(m, dims=2, temp=temp)\n\n\n\n\n\n","category":"function"},{"location":"api/arithmetics/#AcceleratedKernels.minimum","page":"Arithmetics","title":"AcceleratedKernels.minimum","text":"minimum(\n    src::AbstractArray, backend::Backend=get_backend(src);\n    init=typemax(eltype(src)),\n    dims::Union{Nothing, Int}=nothing,\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n    switch_below::Int=0,\n)\n\nMinimum of elements of an array, with optional init and dims. Arguments are the same as for reduce.\n\nExamples\n\nSimple minimum of elements in a vector:\n\nimport AcceleratedKernels as AK\nusing CUDA\n\nv = CuArray(rand(Int32(1):Int32(100), 100_000))\nm = AK.minimum(v)\n\nRow-wise minimum of a matrix:\n\nm = CuArray(rand(Int32(1):Int32(100), 10, 100_000))\nm = AK.minimum(m, dims=1)\n\nIf you know the shape of the resulting array (in case of a axis-wise minimum, i.e. dims is not nothing), you can provide the temp argument to save results into and avoid allocations:\n\nm = CuArray(rand(Int32(1):Int32(100), 10, 100_000))\ntemp = CuArray(ones(Int32, 10))\nm = AK.minimum(m, dims=2, temp=temp)\n\n\n\n\n\n","category":"function"},{"location":"api/arithmetics/#AcceleratedKernels.maximum","page":"Arithmetics","title":"AcceleratedKernels.maximum","text":"maximum(\n    src::AbstractArray, backend::Backend=get_backend(src);\n    init=typemin(eltype(src)),\n    dims::Union{Nothing, Int}=nothing,\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n    switch_below::Int=0,\n)\n\nMaximum of elements of an array, with optional init and dims. Arguments are the same as for reduce.\n\nExamples\n\nSimple maximum of elements in a vector:\n\nimport AcceleratedKernels as AK\nusing oneAPI\n\nv = oneArray(rand(Int32(1):Int32(100), 100_000))\nm = AK.maximum(v)\n\nRow-wise maximum of a matrix:\n\nm = oneArray(rand(Int32(1):Int32(100), 10, 100_000))\nm = AK.maximum(m, dims=1)\n\nIf you know the shape of the resulting array (in case of a axis-wise maximum, i.e. dims is not nothing), you can provide the temp argument to save results into and avoid allocations:\n\nm = oneArray(rand(Int32(1):Int32(100), 10, 100_000))\ntemp = oneArray(zeros(Int32, 10))\nm = AK.maximum(m, dims=2, temp=temp)\n\n\n\n\n\n","category":"function"},{"location":"api/arithmetics/#AcceleratedKernels.count","page":"Arithmetics","title":"AcceleratedKernels.count","text":"count(\n    [f=identity], src::AbstractArray, backend::Backend=get_backend(src);\n    init=0,\n    dims::Union{Nothing, Int}=nothing,\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n    switch_below::Int=0,\n)\n\nCount the number of elements in src for which the function f returns true. If f is omitted, count the number of true elements in src. Arguments are the same as for mapreduce.\n\nExamples\n\nSimple count of true elements in a vector:\n\nimport AcceleratedKernels as AK\nusing Metal\n\nv = MtlArray(rand(Bool, 100_000))\nc = AK.count(v)\n\nCount of elements greater than 50 in a vector:\n\nv = MtlArray(rand(Int32(1):Int32(100), 100_000))\nc = AK.count(x -> x > 50, v)\n\nRow-wise count of true elements in a matrix:\n\nm = MtlArray(rand(Bool, 10, 100_000))\nc = AK.count(m, dims=1)\n\nIf you know the shape of the resulting array (in case of a axis-wise count, i.e. dims is not nothing), you can provide the temp argument to save results into and avoid allocations:\n\nm = MtlArray(rand(Bool, 10, 100_000))\ntemp = MtlArray(zeros(Int32, 10))\nc = AK.count(m, dims=2, temp=temp)\n\n\n\n\n\n","category":"function"},{"location":"api/arithmetics/#AcceleratedKernels.cumsum","page":"Arithmetics","title":"AcceleratedKernels.cumsum","text":"cumsum(\n    src::AbstractArray, backend::Backend=get_backend(src);\n    init=zero(eltype(src)),\n    neutral=zero(eltype(src)),\n    dims::Union{Nothing, Int}=nothing,\n\n    # Algorithm choice\n    alg::AccumulateAlgorithm=DecoupledLookback(),\n\n    # GPU settings\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n    temp_flags::Union{Nothing, AbstractArray}=nothing,\n)\n\nCumulative sum of elements of an array, with optional init and dims. Arguments are the same as for accumulate.\n\nPlatform-Specific Notes\n\nOn Apple Metal, the alg=ScanPrefixes() algorithm is used by default.\n\nExamples\n\nSimple cumulative sum of elements in a vector:\n\nimport AcceleratedKernels as AK\nusing AMDGPU\n\nv = ROCArray(rand(Int32(1):Int32(100), 100_000))\ns = AK.cumsum(v)\n\nRow-wise cumulative sum of a matrix:\n\nm = ROCArray(rand(Int32(1):Int32(100), 10, 100_000))\ns = AK.cumsum(m, dims=1)\n\n\n\n\n\n","category":"function"},{"location":"api/arithmetics/#AcceleratedKernels.cumprod","page":"Arithmetics","title":"AcceleratedKernels.cumprod","text":"cumprod(\n    src::AbstractArray, backend::Backend=get_backend(src);\n    init=one(eltype(src)),\n    neutral=one(eltype(src)),\n    dims::Union{Nothing, Int}=nothing,\n\n    # Algorithm choice\n    alg::AccumulateAlgorithm=DecoupledLookback(),\n\n    # GPU settings\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n    temp_flags::Union{Nothing, AbstractArray}=nothing,\n)\n\nCumulative product of elements of an array, with optional init and dims. Arguments are the same as for accumulate.\n\nPlatform-Specific Notes\n\nOn Apple Metal, the alg=ScanPrefixes() algorithm is used by default.\n\nExamples\n\nSimple cumulative product of elements in a vector:\n\nimport AcceleratedKernels as AK\nusing oneAPI\n\nv = oneArray(rand(Int32(1):Int32(100), 100_000))\np = AK.cumprod(v)\n\nRow-wise cumulative product of a matrix:\n\nm = oneArray(rand(Int32(1):Int32(100), 10, 100_000))\np = AK.cumprod(m, dims=1)\n\n\n\n\n\n","category":"function"},{"location":"debugging/#Debugging-Kernels","page":"Debugging Kernels","title":"Debugging Kernels","text":"","category":"section"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"As the compilation pipeline of GPU kernels is different to that of base Julia, error messages also look different - for example, where Julia would insert an exception when a variable name was not defined (e.g. we had a typo), a GPU kernel throwing exceptions cannot be compiled and instead you'll see some cascading errors like \"[...] compiling [...] resulted in invalid LLVM IR\" caused by \"Reason: unsupported use of an undefined name\" resulting in \"Reason: unsupported dynamic function invocation\", etc.","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"Thankfully, there are only about 3 types of such error messages and they're not that scary when you look into them.","category":"page"},{"location":"debugging/#Undefined-Variables-/-Typos","page":"Debugging Kernels","title":"Undefined Variables / Typos","text":"","category":"section"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"If you misspell a variable name, Julia would insert an exception:","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"function set_color(v, color)\n    AK.foreachindex(v) do i\n        v[i] = colour           # Grab your porridge\n    end\nend","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"However, exceptions cannot be compiled on GPUs and you will see cascading errors like below:","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"(Image: Undefined Name Error)","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"The key thing to look for is undefined name, then search for it in your code.","category":"page"},{"location":"debugging/#Exceptions-and-Checks-that-throw","page":"Debugging Kernels","title":"Exceptions and Checks that throw","text":"","category":"section"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"As mentioned above, exceptions cannot be compiled in GPU kernels; however, many normal-looking functions that we reference in kernels may contain argument-checking. If it cannot be proved that a check branch would not throw an exception, you will see a similar cascade of errors. For example, casting a Float32 to an Int32 includes an InexactError exception check - see this tame-looking code:","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"function mymul!(v)\n    AK.foreachindex(v) do i\n        v[i] *= 2f0\n    end\nend\n\nv = MtlArray(1:1000)\nmymul!(v)","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"See any problem with it? The MtlArray(1:1000) creates a GPU vector filled with Int64 values, but within foreachindex we do v[i] *= 2.0. We are multiplying an Int64 by a Float32, resulting in a Float32 value that we try to write back into v - this may throw an exception, like in normal Julia code:","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"julia> x = [1, 2, 3];\njulia> x[1] = 42.5\nERROR: InexactError: Int64(42.5)","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"On GPUs you will see an error like this:","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"(Image: Check Exception Error)","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"Note the error stack: setindex!, convert, Int64, box_float32 - because of the exception check, we have a type instability, which in turn results in boxing values behind pointers, in turn resulting in dynamic memory allocation and finally the error we see at the top, unsupported call to gpu_malloc.","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"You may need to do your correctness checks manually, without exceptions; in this specific case, if we did want to cast a Float32 to an Int, we could use unsafe_trunc(T, x) - though be careful when using unsafe functions that you understand their behaviour and assumptions (e.g. log has a DomainError check for negative values):","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"function mymul!(v)\n    AK.foreachindex(v) do i\n        v[i] = unsafe_trunc(eltype(v), v[i] * 2.5f0)\n    end\nend\n\nv = MtlArray(1:1000)\nmymul!(v)","category":"page"},{"location":"debugging/#Type-Instability-/-Global-Variables","page":"Debugging Kernels","title":"Type Instability / Global Variables","text":"","category":"section"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"Types must be known to be captured and compiled within GPU kernels. Global variables without const are not type-stable, as you could associate a different value later on in a script:","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"v = MtlArray(1:1000)\n\nAK.foreachindex(v) do i\n    v[i] *= 2\nend\n\nv = \"potato\"","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"The error stack is a bit more difficult here:","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"(Image: Type Unstable Error)","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"You see a few dynamic function invocation, an unsupported call to gpu_malloc, and a bit further down a box. The more operations you do on the type-unstable object, the more dynamic function invocation errors you'll see. These would also be the steps Base Julia would take to allow dynamically-changing objects: they'd be put in a Box behind pointers, and allocated on the heap. In a way, it is better that we cannot do that on a GPU, as it hurts performance massively.","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"Solving this is easy - stick the foreachindex in a function where the variable types are known at compile-time:","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"function mymul!(v, x)\n    AK.foreachindex(v) do i\n        v[i] *= x\n    end\nend\n\nv = MtlArray(1:1000)\nmymul!(v, 2)","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"Note that Julia's lambda capture is very powerful - inside AK.foreachindex you can references other objects from within the function (like x), without explicitly passing them to the GPU.","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"Note that while technically we could write const v in the global scope, when writing a closure (in a do ... end block) that references outer objects, the closure may try to capture other variables defined in the current Julia session; as such, it is an unreliable approach for GPU kernels (example issue) that we do not recommend. Use functions.","category":"page"},{"location":"debugging/#Apple-Metal-Only:-Float64-is-not-Supported","page":"Debugging Kernels","title":"Apple Metal Only: Float64 is not Supported","text":"","category":"section"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"Mac GPUs do not natively support Float64 values; there is a high-level check when trying to create an array:","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"julia> x = MtlArray([1.0, 2.0, 3.0])\nERROR: Metal does not support Float64 values, try using Float32 instead","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"However, if we tried to use / convert values in a kernel to a Float64:","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"function mymul!(v, x)\n    AK.foreachindex(v) do i\n        v[i] *= x\n    end\nend\n\nv = MtlArray{Float32}(1:1000)\nmymul!(v, 2.0)","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"Note that we try to multiply Float32 values by 2.0, which is a Float64 - in which case we get:","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"ERROR: LoadError: Compilation to native code failed; see below for details.\n[...]\ncaused by: NSError: Compiler encountered an internal error (AGXMetalG15X_M1, code 3)\n[...]","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"Change the 2.0 to 2.0f0 or Float32(2); in kernels with generic types (that are supposed to work on multiple possible input types), do use the same types as your inputs, using e.g. T = eltype(v) then zero(T), T(42), etc.","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"","category":"page"},{"location":"debugging/","page":"Debugging Kernels","title":"Debugging Kernels","text":"For other library-related problems, feel free to post a GitHub issue. For help implementing new code, or just advice, you can also use the Julia Discourse forum, the community is incredibly helpful.","category":"page"},{"location":"api/predicates/#Predicates","page":"Predicates","title":"Predicates","text":"","category":"section"},{"location":"api/predicates/","page":"Predicates","title":"Predicates","text":"Apply a predicate to check if all / any elements in a collection return true. Could be implemented as a reduction, but is better optimised with stopping the search once a false / true is found.","category":"page"},{"location":"api/predicates/","page":"Predicates","title":"Predicates","text":"Other names: not often implemented standalone on GPUs, typically included as part of a reduction.","category":"page"},{"location":"api/predicates/#AcceleratedKernels.any","page":"Predicates","title":"AcceleratedKernels.any","text":"any(\n    pred, v::AbstractArray, backend::Backend=get_backend(v);\n\n    # Algorithm choice\n    alg::PredicatesAlgorithm=ConcurrentWrite(),\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size::Int=256,\n)\n\nCheck if any element of v satisfies the predicate pred (i.e. some pred(v[i]) == true). Optimised differently to mapreduce due to shortcircuiting behaviour of booleans.\n\nOther names: not often implemented standalone on GPUs, typically included as part of a reduction.\n\nCPU\n\nMultithreaded parallelisation is only worth it for large arrays, relatively expensive predicates, and/or rare occurrence of true; use max_tasks and min_elems to only use parallelism when worth it in your application. When only one thread is needed, there is no overhead.\n\nGPU\n\nThere are two possible alg choices:\n\nConcurrentWrite(): the default algorithm, using concurrent writing to a global flag; there is only one platform we are aware of (Intel UHD 620 integrated graphics cards) where multiple threads writing to the same memory location - even if writing the same value - hang the device.\nMapReduce(; temp=nothing, switch_below=0): a conservative mapreduce-based implementation which can be used on all platforms, but does not use shortcircuiting optimisations. You can set the temp and switch_below keyword arguments to be forwarded to mapreduce.\n\nPlatform-Specific Notes\n\nOn oneAPI, alg=MapReduce() is the default as on some Intel GPUs concurrent global writes hang the device.\n\nExamples\n\nimport AcceleratedKernels as AK\nusing CUDA\n\nv = CuArray(rand(Float32, 100_000))\nAK.any(x -> x < 1, v)\n\nUsing a different algorithm:\n\nAK.any(x -> x < 1, v, alg=AK.MapReduce(switch_below=100))\n\nChecking a more complex condition with unmaterialised index ranges:\n\nfunction complex_any(x, y)\n    AK.any(eachindex(x), AK.get_backend(x)) do i\n        x[i] < 0 && y[i] > 0\n    end\nend\n\ncomplex_any(CuArray(rand(Float32, 100)), CuArray(rand(Float32, 100)))\n\n\n\n\n\n","category":"function"},{"location":"api/predicates/#AcceleratedKernels.all","page":"Predicates","title":"AcceleratedKernels.all","text":"all(\n    pred, v::AbstractArray, backend::Backend=get_backend(v);\n\n    # Algorithm choice\n    alg::PredicatesAlgorithm=ConcurrentWrite(),\n\n    # CPU settings\n    max_tasks=Threads.nthreads(),\n    min_elems=1,\n\n    # GPU settings\n    block_size::Int=256,\n)\n\nCheck if all elements of v satisfy the predicate pred (i.e. all pred(v[i]) == true). Optimised differently to mapreduce due to shortcircuiting behaviour of booleans.\n\nOther names: not often implemented standalone on GPUs, typically included as part of a reduction.\n\nCPU\n\nMultithreaded parallelisation is only worth it for large arrays, relatively expensive predicates, and/or rare occurrence of true; use max_tasks and min_elems to only use parallelism when worth it in your application. When only one thread is needed, there is no overhead.\n\nGPU\n\nThere are two possible alg choices:\n\nConcurrentWrite(): the default algorithm, using concurrent writing to a global flag; there is only one platform we are aware of (Intel UHD 620 integrated graphics cards) where multiple threads writing to the same memory location - even if writing the same value - hang the device.\nMapReduce(; temp=nothing, switch_below=0): a conservative mapreduce-based implementation which can be used on all platforms, but does not use shortcircuiting optimisations. You can set the temp and switch_below keyword arguments to be forwarded to mapreduce.\n\nPlatform-Specific Notes\n\nOn oneAPI, alg=MapReduce() is the default as on some Intel GPUs concurrent global writes hang the device.\n\nExamples\n\nimport AcceleratedKernels as AK\nusing Metal\n\nv = MtlArray(rand(Float32, 100_000))\nAK.all(x -> x > 0, v)\n\nUsing a different algorithm:\n\nAK.all(x -> x > 0, v, alg=AK.MapReduce(switch_below=100))\n\nChecking a more complex condition with unmaterialised index ranges:\n\nfunction complex_all(x, y)\n    AK.all(eachindex(x), AK.get_backend(x)) do i\n        x[i] > 0 && y[i] < 0\n    end\nend\n\ncomplex_all(CuArray(rand(Float32, 100)), CuArray(rand(Float32, 100)))\n\n\n\n\n\n","category":"function"},{"location":"api/predicates/","page":"Predicates","title":"Predicates","text":"Note on the cooperative keyword: some older platforms crash when multiple threads write to the same memory location in a global array (e.g. old Intel Graphics); if all threads were to write the same value, it is well-defined on others (e.g. CUDA F4.2 says \"If a non-atomic instruction executed by a warp writes to the same location in global memory for more than one of the threads of the warp, only one thread performs a write and which thread does it is undefined.\"). This \"cooperative\" thread behaviour allows for a faster implementation; if you have a platform - the only one I know is Intel UHD Graphics - that crashes, set cooperative=false to use a safer mapreduce-based implementation.","category":"page"},{"location":"testing/#Testing","page":"Testing","title":"Testing","text":"","category":"section"},{"location":"testing/","page":"Testing","title":"Testing","text":"import AcceleratedKernels as AK # hide\nAK.DocHelpers.readme_section(\"## 7. Testing\") # hide","category":"page"},{"location":"api/mapreduce/#MapReduce","page":"MapReduce","title":"MapReduce","text":"","category":"section"},{"location":"api/mapreduce/","page":"MapReduce","title":"MapReduce","text":"Equivalent to reduce(op, map(f, iterable)), without saving the intermediate mapped collection; can be used to e.g. split documents into words (map) and count the frequency thereof (reduce).","category":"page"},{"location":"api/mapreduce/","page":"MapReduce","title":"MapReduce","text":"Other names: transform_reduce, some fold implementations include the mapping function too.","category":"page"},{"location":"api/mapreduce/","page":"MapReduce","title":"MapReduce","text":"","category":"page"},{"location":"api/mapreduce/#AcceleratedKernels.mapreduce","page":"MapReduce","title":"AcceleratedKernels.mapreduce","text":"mapreduce(\n    f, op, src::AbstractArray, backend::Backend=get_backend(src);\n    init,\n    neutral=neutral_element(op, eltype(src)),\n    dims::Union{Nothing, Int}=nothing,\n\n    # CPU settings\n    max_tasks::Int=Threads.nthreads(),\n    min_elems::Int=1,\n\n    # GPU settings\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n    switch_below::Int=0,\n)\n\nReduce src along dimensions dims using the binary operator op after applying f elementwise. If dims is nothing, reduce src to a scalar. If dims is an integer, reduce src along that dimension. The init value is used as the initial value for the reduction (i.e. after mapping).\n\nThe neutral value is the neutral element (zero) for the operator op, which is needed for an efficient GPU implementation that also allows a nonzero init.\n\nThe returned type is the same as init - to control output precision, specify init explicitly.\n\nCPU settings\n\nUse at most max_tasks threads with at least min_elems elements per task. For N-dimensional arrays (dims::Int) multithreading currently only becomes faster for max_tasks >= 4; all other cases are scaling linearly with the number of threads.\n\nGPU settings\n\nThe block_size parameter controls the number of threads per block.\n\nThe temp parameter can be used to pass a pre-allocated temporary array. For reduction to a scalar (dims=nothing), length(temp) >= 2 * (length(src) + 2 * block_size - 1) ÷ (2 * block_size) is required. For reduction along a dimension (dims is an integer), temp is used as the destination array, and thus must have the exact dimensions required - i.e. same dimensionwise sizes as src, except for the reduced dimension which becomes 1; there are some corner cases when one dimension is zero, check against Base.reduce for CPU arrays for exact behavior.\n\nThe switch_below parameter controls the threshold below which the reduction is performed on the CPU and is only used for 1D reductions (i.e. dims=nothing).\n\nExample\n\nComputing a sum of squares, reducing down to a scalar that is copied to host:\n\nimport AcceleratedKernels as AK\nusing CUDA\n\nv = CuArray{Int16}(rand(1:1000, 100_000))\nvsumsq = AK.mapreduce(x -> x * x, (x, y) -> x + y, v; init=zero(eltype(v)))\n\nComputing dimensionwise sums of squares in a 2D matrix:\n\nimport AcceleratedKernels as AK\nusing Metal\n\nf(x) = x * x\nm = MtlArray(rand(Int32(1):Int32(100), 10, 100_000))\nmrowsumsq = AK.mapreduce(f, +, m; init=zero(eltype(m)), dims=1)\nmcolsumsq = AK.mapreduce(f, +, m; init=zero(eltype(m)), dims=2)\n\n\n\n\n\n","category":"function"},{"location":"api/using_backends/#Using-Different-Backends","page":"Using Different Backends","title":"Using Different Backends","text":"","category":"section"},{"location":"api/using_backends/","page":"Using Different Backends","title":"Using Different Backends","text":"For any of the examples here, simply use a different GPU array and AcceleratedKernels.jl will pick the right backend:","category":"page"},{"location":"api/using_backends/","page":"Using Different Backends","title":"Using Different Backends","text":"# Intel Graphics\nusing oneAPI\nv = oneArray{Int32}(undef, 100_000)             # Empty array\n\n# AMD ROCm\nusing AMDGPU\nv = ROCArray{Float64}(1:100_000)                # A range converted to Float64\n\n# Apple Metal\nusing Metal\nv = MtlArray(rand(Float32, 100_000))            # Transfer from host to device\n\n# NVidia CUDA\nusing CUDA\nv = CuArray{UInt32}(0:5:100_000)                # Range with explicit step size\n\n# Transfer GPU array back\nv_host = Array(v)","category":"page"},{"location":"api/using_backends/","page":"Using Different Backends","title":"Using Different Backends","text":"All publicly-exposed functions have CPU implementations with unified parameter interfaces:","category":"page"},{"location":"api/using_backends/","page":"Using Different Backends","title":"Using Different Backends","text":"import AcceleratedKernels as AK\nv = Vector(-1000:1000)                          # Normal CPU array\nAK.reduce(+, v, max_tasks=Threads.nthreads())","category":"page"},{"location":"api/using_backends/","page":"Using Different Backends","title":"Using Different Backends","text":"By default all algorithms use the number of threads Julia was started with.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"(Image: Logo)","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Parallel algorithm building blocks for the Julia ecosystem, targeting multithreaded CPUs, and GPUs via Intel oneAPI, AMD ROCm, Apple Metal and Nvidia CUDA (and any future backends added to the JuliaGPU organisation).","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"","category":"page"},{"location":"#What's-Different?","page":"Overview","title":"What's Different?","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"import AcceleratedKernels as AK # hide\nAK.DocHelpers.readme_section(\"## 1. What's Different?\") # hide","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"","category":"page"},{"location":"#Status","page":"Overview","title":"Status","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"import AcceleratedKernels as AK # hide\nAK.DocHelpers.readme_section(\"## 2. Status\") # hide","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"","category":"page"},{"location":"#Acknowledgements","page":"Overview","title":"Acknowledgements","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"import AcceleratedKernels as AK # hide\nAK.DocHelpers.readme_section(\"## 11. Acknowledgements\") # hide","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"","category":"page"},{"location":"#License","page":"Overview","title":"License","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"AcceleratedKernels.jl is MIT-licensed. Enjoy.","category":"page"},{"location":"api/reduce/#Reductions","page":"Reduce","title":"Reductions","text":"","category":"section"},{"location":"api/reduce/","page":"Reduce","title":"Reduce","text":"Apply a custom binary operator reduction on all elements in an iterable; can be used to compute minima, sums, counts, etc.","category":"page"},{"location":"api/reduce/","page":"Reduce","title":"Reduce","text":"Other names: Kokkos:parallel_reduce, fold, aggregate.","category":"page"},{"location":"api/reduce/","page":"Reduce","title":"Reduce","text":"","category":"page"},{"location":"api/reduce/#AcceleratedKernels.reduce","page":"Reduce","title":"AcceleratedKernels.reduce","text":"reduce(\n    op, src::AbstractArray, backend::Backend=get_backend(src);\n    init,\n    neutral=neutral_element(op, eltype(src)),\n    dims::Union{Nothing, Int}=nothing,\n\n    # CPU settings\n    max_tasks::Int=Threads.nthreads(),\n    min_elems::Int=1,\n\n    # GPU settings\n    block_size::Int=256,\n    temp::Union{Nothing, AbstractArray}=nothing,\n    switch_below::Int=0,\n)\n\nReduce src along dimensions dims using the binary operator op. If dims is nothing, reduce src to a scalar. If dims is an integer, reduce src along that dimension. The init value is used as the initial value for the reduction; neutral is the neutral element for the operator op.\n\nThe returned type is the same as init - to control output precision, specify init explicitly.\n\nCPU settings\n\nUse at most max_tasks threads with at least min_elems elements per task. For N-dimensional arrays (dims::Int) multithreading currently only becomes faster for max_tasks >= 4; all other cases are scaling linearly with the number of threads.\n\nNote that multithreading reductions only improves performance for cases with more compute-heavy operations, which hide the memory latency and thread launch overhead - that includes:\n\nReducing more complex types, e.g. reduction of tuples / structs / strings.\nMore complex operators, e.g. op=custom_complex_op_function.\n\nFor non-memory-bound operations, reductions scale almost linearly with the number of threads.\n\nGPU settings\n\nThe block_size parameter controls the number of threads per block.\n\nThe temp parameter can be used to pass a pre-allocated temporary array. For reduction to a scalar (dims=nothing), length(temp) >= 2 * (length(src) + 2 * block_size - 1) ÷ (2 * block_size) is required. For reduction along a dimension (dims is an integer), temp is used as the destination array, and thus must have the exact dimensions required - i.e. same dimensionwise sizes as src, except for the reduced dimension which becomes 1; there are some corner cases when one dimension is zero, check against Base.reduce for CPU arrays for exact behavior.\n\nThe switch_below parameter controls the threshold below which the reduction is performed on the CPU and is only used for 1D reductions (i.e. dims=nothing).\n\nExamples\n\nComputing a sum, reducing down to a scalar that is copied to host:\n\nimport AcceleratedKernels as AK\nusing CUDA\n\nv = CuArray{Int16}(rand(1:1000, 100_000))\nvsum = AK.reduce((x, y) -> x + y, v; init=zero(eltype(v)))\n\nComputing dimensionwise sums in a 2D matrix:\n\nimport AcceleratedKernels as AK\nusing Metal\n\nm = MtlArray(rand(Int32(1):Int32(100), 10, 100_000))\nmrowsum = AK.reduce(+, m; init=zero(eltype(m)), dims=1)\nmcolsum = AK.reduce(+, m; init=zero(eltype(m)), dims=2)\n\n\n\n\n\n","category":"function"}]
}
